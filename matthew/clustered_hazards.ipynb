{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lifelines import CoxTimeVaryingFitter\n",
    "\n",
    "from sksurv.ensemble import RandomSurvivalForest\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from lifelines import KaplanMeierFitter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_first_target_times(df, patient_col=\"ID\", time_col=\"Month\", targets=[\"event_vl50\", \"event_vl200\", \"event_cd4\"]):\n",
    "    \"\"\"\n",
    "    Finds the first time each patient reaches the given targets.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Original DataFrame with patient data.\n",
    "    - patient_col (str): Name of the patient ID column.\n",
    "    - time_col (str): Name of the time column.\n",
    "    - targets (list): List of target event columns to check.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A DataFrame containing Patient ID and first time they hit each target.\n",
    "    \"\"\"\n",
    "    result_df = pd.DataFrame()\n",
    "\n",
    "    # Loop through each target and find first occurrence\n",
    "    for target in targets:\n",
    "        first_times = df[df[target] == 1].groupby(patient_col)[time_col].min().reset_index()\n",
    "        first_times.rename(columns={time_col: f\"first_{target}\"}, inplace=True)\n",
    "        \n",
    "        # Merge into result DataFrame\n",
    "        if result_df.empty:\n",
    "            result_df = first_times\n",
    "        else:\n",
    "            result_df = result_df.merge(first_times, on=patient_col, how=\"outer\")\n",
    "\n",
    "    return result_df\n",
    "\n",
    "def map_regimen(row):\n",
    "    \"\"\"Combine regimen components into a standardized name.\"\"\"\n",
    "    drug_mappings = {\n",
    "    'Base_Drug_Combo': {\n",
    "        0: 'FTC + TDF',\n",
    "        1: '3TC + ABC',\n",
    "        2: 'FTC + TAF',\n",
    "        3: 'DRV + FTC + TDF',\n",
    "        4: 'FTC + RTVB + TDF',\n",
    "        5: 'Other'\n",
    "    },\n",
    "    'Comp_INI': {0: 'DTG', 1: 'RAL', 2: 'EVG', 3: 'N/A'},\n",
    "    'Comp_NNRTI': {0: 'NVP', 1: 'EFV', 2: 'RPV', 3: 'N/A'},\n",
    "    'ExtraPI': {0: 'DRV', 1: 'RTVB', 2: 'LPV', 3: 'RTV', 4: 'ATV', 5: 'N/A'},\n",
    "}\n",
    "\n",
    "    base = drug_mappings['Base_Drug_Combo'].get(row['Base_Drug_Combo'], 'Unknown')\n",
    "    ini = drug_mappings['Comp_INI'].get(row['Comp_INI'], '')\n",
    "    nnrti = drug_mappings['Comp_NNRTI'].get(row['Comp_NNRTI'], '')\n",
    "    pi = drug_mappings['ExtraPI'].get(row['ExtraPI'], '')\n",
    "    enhancer = ' + ExtraPK' if row['ExtraPk_En'] == 1 else ''\n",
    "    components = [base, ini, nnrti, pi]\n",
    "    regimen = ' + '.join(filter(None, components)) + enhancer\n",
    "    return regimen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('BDHSC_SCC_2025_synth_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Regimen'] = df.apply(map_regimen, axis=1)\n",
    "df['event_vl50'] = df['VL'] < 50\n",
    "df['event_vl250'] = df['VL'] < 250\n",
    "df['event_cd4'] = df['CD4'] > 500\n",
    "targets = [\"event_vl50\", \"event_vl250\", \"event_cd4\"]\n",
    "first_target_times_df = get_first_target_times(df, targets=targets)\n",
    "df = pd.merge(df, first_target_times_df, on='ID', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sorted = df.sort_values(['ID', 'Month'])\n",
    "df_sorted['Regimen_Shift'] = (df_sorted.groupby('ID')['Regimen'].shift() != df_sorted['Regimen']).astype(int)\n",
    "df_sorted['Regimen_Start'] = df_sorted.groupby('ID')['Regimen_Shift'].cumsum()\n",
    "intervals = df_sorted.groupby(['ID', 'Regimen_Start']).agg(\n",
    "    Start=('Month', 'min'),\n",
    "    End=('Month', 'max'),\n",
    "    Regimen=('Regimen', 'first'),\n",
    "    VL=('VL', lambda x: list(x)),\n",
    "    CD4=('CD4', lambda x: list(x)),\n",
    "    VL_50_time=('first_event_vl50', 'first'),\n",
    "    VL_250_time=('first_event_vl250', 'first'),\n",
    "    CD4_500_time=('first_event_cd4', 'first'),\n",
    "    Gender=('Gender', 'first'),\n",
    "    Ethnicity=('Ethnic', 'first'),\n",
    "    VL_50_baseline=('VL', 'first'),\n",
    "    CD4_500_baseline=('CD4', 'first'),\n",
    "    CD4_Percent_baseline=('RelCD4', 'first')\n",
    ").reset_index()\n",
    "intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_censored = intervals.copy()\n",
    "intervals_censored['VL_50_Censored'] = np.where(\n",
    "    ~((intervals_censored['Start'] <= intervals_censored['VL_50_time']) & \n",
    "      (intervals_censored['VL_50_time'] <= intervals_censored['End'])),\n",
    "    True,  # If the condition is not met, mark as censored (True)\n",
    "    False  # Otherwise, mark as not censored (False)\n",
    ")\n",
    "intervals_censored['VL_250_Censored'] = np.where(\n",
    "    ~((intervals_censored['Start'] <= intervals_censored['VL_250_time']) & \n",
    "      (intervals_censored['VL_250_time'] <= intervals_censored['End'])),\n",
    "    True,  # If the condition is not met, mark as censored (True)\n",
    "    False  # Otherwise, mark as not censored (False)\n",
    ")\n",
    "intervals_censored['CD4_500_Censored'] = np.where(\n",
    "    ~((intervals_censored['Start'] <= intervals_censored['CD4_500_time']) & \n",
    "      (intervals_censored['CD4_500_time'] <= intervals_censored['End'])),\n",
    "    True,  # If the condition is not met, mark as censored (True)\n",
    "    False  # Otherwise, mark as not censored (False)\n",
    ")\n",
    "intervals_censored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_censored.to_csv('intervals_censored.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_intervals(df, mode):\n",
    "    \"\"\"Filter intervals where start time < target time (or target not achieved).\"\"\"\n",
    "\n",
    "    if mode == 'vl50':\n",
    "        mask = (df['Start'] < df['VL_50_time'])\n",
    "    elif mode == 'vl250':\n",
    "        mask = (df['Start'] < df['VL_250_time'])\n",
    "    else:\n",
    "        mask = (df['Start'] < df['CD4_500_time'])\n",
    "    df = df[mask]\n",
    "    return df[df['End'] - df['Start'] >= 3]\n",
    "\n",
    "def cox(mode, intervals_censored, verbose=False):\n",
    "    filtered_intervals_outcomes = filter_intervals(intervals_censored, mode)\n",
    "\n",
    "    tv_data = []\n",
    "    for _, row in filtered_intervals_outcomes.iterrows():\n",
    "        start = row['Start']\n",
    "        if mode == 'vl50':\n",
    "            end = row['End'] if row['VL_50_Censored'] == 1 else row['VL_50_time']\n",
    "        elif mode == 'vl250':\n",
    "            end = row['End'] if row['VL_250_Censored'] == 1 else row['VL_250_time']\n",
    "        else:\n",
    "            end = row['End'] if row['CD4_500_Censored'] == 1 else row['CD4_500_time']\n",
    "        \n",
    "        tv_data.append({\n",
    "            'Patient ID': row['ID'],\n",
    "            'start': start,\n",
    "            'stop': end,\n",
    "            'gender': row['Gender'],\n",
    "            'ethnicity': row['Ethnicity'],\n",
    "            'censor': row['VL_50_Censored'] if mode == 'vl50' else row['VL_250_Censored'] if mode == 'vl250' else row['CD4_500_Censored'],\n",
    "            'Regimen': row['Regimen'],\n",
    "            'Baseline_VL': row['VL_50_baseline'],\n",
    "            'Baseline_CD4': row['CD4_500_baseline'],\n",
    "            'Baseline_CD4_Percent': row['CD4_Percent_baseline']\n",
    "        })\n",
    "\n",
    "    \n",
    "    tv_df = pd.DataFrame(tv_data)\n",
    "    counts = tv_df[\"Regimen\"].value_counts()\n",
    "    common_regimens = counts[counts > len(filtered_intervals_outcomes)/40].index\n",
    "\n",
    "    tv_df[\"Regimen\"] = tv_df[\"Regimen\"].apply(\n",
    "        lambda x: x if x in common_regimens else \"Other\"\n",
    "    )\n",
    "\n",
    "    # tv_df[\"gender\"] = tv_df[\"gender\"].astype(\"category\")\n",
    "    # tv_df[\"ethnicity\"] = tv_df[\"ethnicity\"].astype(\"category\")\n",
    "    tv_df = pd.get_dummies(tv_df, columns=[\"Regimen\"], drop_first=True)\n",
    "\n",
    "    tv_df.drop([\"gender\", 'ethnicity'], axis=1, inplace=True)\n",
    "    if 'Regimen_Other' in tv_df.columns:\n",
    "        tv_df.drop(['Regimen_Other'], axis=1, inplace=True)\n",
    "\n",
    "    ctv = CoxTimeVaryingFitter()\n",
    "    ctv.fit(\n",
    "        tv_df,\n",
    "        id_col=\"Patient ID\",\n",
    "        event_col=\"censor\",\n",
    "        start_col=\"start\",\n",
    "        stop_col=\"stop\",\n",
    "        show_progress=True,\n",
    "        fit_options={'precision': 1e-4, 'r_precision': 1e-4}\n",
    "    )\n",
    "    if verbose:\n",
    "        ctv.print_summary()\n",
    "\n",
    "    coeffs = ctv.summary\n",
    "    # coeffs = coeffs[coeffs['p'] < 0.05]\n",
    "    for baseline in ['Baseline_VL', 'Baseline_CD4', 'Baseline_CD4_Percent']:\n",
    "        if baseline in coeffs.index:\n",
    "            coeffs = coeffs.drop(index=[baseline])\n",
    "\n",
    "    coeffs = coeffs.reset_index()\n",
    "    coeffs = coeffs[['covariate', 'exp(coef)']]\n",
    "    coeffs.columns = ['covariate', f'effect_{mode}']\n",
    "    \n",
    "    return coeffs\n",
    "\n",
    "def optimal_regimen(intervals_censored):\n",
    "    coeffs_tv50 = cox('vl50', intervals_censored)\n",
    "    coeffs_tv250 = cox('vl250', intervals_censored)\n",
    "    coeffs_cd500 = cox('cd500', intervals_censored)\n",
    "\n",
    "    df = pd.merge(coeffs_tv50, coeffs_tv250, on='covariate', how='inner')\n",
    "    df = pd.merge(df, coeffs_cd500, on='covariate', how='inner')\n",
    "    # df['multiplicative_effect'] = np.power(df['effect_vl50'], weights[0]/denom) * np.power(df['effect_vl250'], weights[0]/denom) * np.power(df['effect_cd500'], weights[1]/denom)\n",
    "    # df = df[df['multiplicative_effect'] >= 1]\n",
    "\n",
    "    # df = df.sort_values(['multiplicative_effect'], ascending=False)\n",
    "    df.columns = ['regimen', 'effect_vl50', 'effect_vl250', 'effect_cd500']\n",
    "    df['regimen'] = df['regimen'].apply(lambda x:x.split('Regimen_')[1])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "intervals_censored = pd.read_csv('intervals_censored.csv')\n",
    "intervals_censored[\"id\"] = range(len(intervals_censored))\n",
    "black_male_cluster = pd.read_csv('black_male_cluster_labels.csv')\n",
    "intervals_censored_black_male = pd.merge(intervals_censored, black_male_cluster, on='id', how='inner')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 6.13e-01, step_size = 0.9500, log_lik = -95465.37163, newton_decrement = 1.19e+03, seconds_since_start = 0.2\n",
      "Iteration 2: norm_delta = 1.94e-01, step_size = 0.9500, log_lik = -94228.66555, newton_decrement = 7.93e+01, seconds_since_start = 0.4\n",
      "Iteration 3: norm_delta = 2.17e-02, step_size = 0.9500, log_lik = -94147.33886, newton_decrement = 7.97e-01, seconds_since_start = 0.5\n",
      "Iteration 4: norm_delta = 1.30e-03, step_size = 1.0000, log_lik = -94146.54097, newton_decrement = 2.52e-03, seconds_since_start = 0.7\n",
      "Iteration 5: norm_delta = 5.39e-07, step_size = 1.0000, log_lik = -94146.53844, newton_decrement = 4.16e-10, seconds_since_start = 0.9\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 5.54e-01, step_size = 0.9500, log_lik = -40672.60550, newton_decrement = 4.57e+02, seconds_since_start = 0.1\n",
      "Iteration 2: norm_delta = 1.35e-01, step_size = 0.9500, log_lik = -40246.09027, newton_decrement = 1.97e+01, seconds_since_start = 0.2\n",
      "Iteration 3: norm_delta = 1.92e-02, step_size = 0.9500, log_lik = -40225.74381, newton_decrement = 3.82e-01, seconds_since_start = 0.3\n",
      "Iteration 4: norm_delta = 1.83e-03, step_size = 1.0000, log_lik = -40225.35451, newton_decrement = 2.90e-03, seconds_since_start = 0.5\n",
      "Iteration 5: norm_delta = 7.35e-06, step_size = 1.0000, log_lik = -40225.35160, newton_decrement = 4.76e-08, seconds_since_start = 0.6\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 1.34e+00, step_size = 0.9500, log_lik = -50813.05090, newton_decrement = 2.17e+03, seconds_since_start = 0.1\n",
      "Iteration 2: norm_delta = 7.50e-01, step_size = 0.9500, log_lik = -48881.07239, newton_decrement = 3.93e+02, seconds_since_start = 0.2\n",
      "Iteration 3: norm_delta = 8.49e-02, step_size = 0.9500, log_lik = -48468.85354, newton_decrement = 1.42e+01, seconds_since_start = 0.3\n",
      "Iteration 4: norm_delta = 8.28e-03, step_size = 1.0000, log_lik = -48454.22899, newton_decrement = 1.89e-01, seconds_since_start = 0.4\n",
      "Iteration 5: norm_delta = 7.30e-05, step_size = 1.0000, log_lik = -48454.03906, newton_decrement = 2.32e-05, seconds_since_start = 0.5\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 5.60e-01, step_size = 0.9500, log_lik = -83526.51567, newton_decrement = 1.16e+03, seconds_since_start = 0.2\n",
      "Iteration 2: norm_delta = 1.45e-01, step_size = 0.9500, log_lik = -82598.86937, newton_decrement = 9.18e+01, seconds_since_start = 0.3\n",
      "Iteration 3: norm_delta = 1.30e-02, step_size = 0.9500, log_lik = -82505.01469, newton_decrement = 1.00e+00, seconds_since_start = 0.5\n",
      "Iteration 4: norm_delta = 8.34e-04, step_size = 1.0000, log_lik = -82504.01062, newton_decrement = 3.62e-03, seconds_since_start = 0.7\n",
      "Iteration 5: norm_delta = 5.76e-07, step_size = 1.0000, log_lik = -82504.00700, newton_decrement = 1.75e-09, seconds_since_start = 1.0\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 6.30e-01, step_size = 0.9500, log_lik = -75316.91252, newton_decrement = 1.08e+03, seconds_since_start = 0.3\n",
      "Iteration 2: norm_delta = 1.57e-01, step_size = 0.9500, log_lik = -74509.28327, newton_decrement = 1.08e+02, seconds_since_start = 0.7\n",
      "Iteration 3: norm_delta = 2.21e-02, step_size = 0.9500, log_lik = -74396.83710, newton_decrement = 2.32e+00, seconds_since_start = 0.9\n",
      "Iteration 4: norm_delta = 1.69e-03, step_size = 1.0000, log_lik = -74394.49697, newton_decrement = 1.17e-02, seconds_since_start = 1.2\n",
      "Iteration 5: norm_delta = 2.77e-06, step_size = 1.0000, log_lik = -74394.48526, newton_decrement = 3.14e-08, seconds_since_start = 1.3\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 9.96e-01, step_size = 0.9500, log_lik = -9910.56003, newton_decrement = 4.01e+02, seconds_since_start = 0.0\n",
      "Iteration 2: norm_delta = 2.80e-01, step_size = 0.9500, log_lik = -9519.76622, newton_decrement = 1.86e+01, seconds_since_start = 0.0\n",
      "Iteration 3: norm_delta = 3.64e-02, step_size = 0.9500, log_lik = -9500.56897, newton_decrement = 4.42e-01, seconds_since_start = 0.0\n",
      "Iteration 4: norm_delta = 3.44e-03, step_size = 1.0000, log_lik = -9500.11936, newton_decrement = 3.07e-03, seconds_since_start = 0.1\n",
      "Iteration 5: norm_delta = 1.15e-05, step_size = 1.0000, log_lik = -9500.11628, newton_decrement = 3.12e-08, seconds_since_start = 0.1\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 9.40e-01, step_size = 0.9500, log_lik = -71678.76866, newton_decrement = 3.04e+03, seconds_since_start = 0.3\n",
      "Iteration 2: norm_delta = 5.92e-01, step_size = 0.9500, log_lik = -69598.62024, newton_decrement = 5.93e+02, seconds_since_start = 0.7\n",
      "Iteration 3: norm_delta = 5.93e-02, step_size = 0.9500, log_lik = -69031.77606, newton_decrement = 5.43e+00, seconds_since_start = 1.2\n",
      "Iteration 4: norm_delta = 2.76e-03, step_size = 1.0000, log_lik = -69026.39439, newton_decrement = 9.03e-03, seconds_since_start = 1.5\n",
      "Iteration 5: norm_delta = 1.78e-06, step_size = 1.0000, log_lik = -69026.38536, newton_decrement = 9.67e-09, seconds_since_start = 1.6\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 1.16e+00, step_size = 0.9500, log_lik = -51222.77366, newton_decrement = 2.25e+03, seconds_since_start = 0.1\n",
      "Iteration 2: norm_delta = 4.71e-01, step_size = 0.9500, log_lik = -49751.49636, newton_decrement = 3.99e+02, seconds_since_start = 0.3\n",
      "Iteration 3: norm_delta = 2.83e-02, step_size = 0.9500, log_lik = -49356.53659, newton_decrement = 1.78e+00, seconds_since_start = 0.4\n",
      "Iteration 4: norm_delta = 1.51e-03, step_size = 1.0000, log_lik = -49354.76550, newton_decrement = 4.19e-03, seconds_since_start = 0.6\n",
      "Iteration 5: norm_delta = 4.69e-07, step_size = 1.0000, log_lik = -49354.76131, newton_decrement = 7.70e-10, seconds_since_start = 0.8\n",
      "Convergence completed after 5 iterations.\n",
      "Iteration 1: norm_delta = 6.44e-01, step_size = 0.9500, log_lik = -17656.11067, newton_decrement = 6.38e+02, seconds_since_start = 0.0\n",
      "Iteration 2: norm_delta = 2.42e-01, step_size = 0.9500, log_lik = -17095.63879, newton_decrement = 2.81e+01, seconds_since_start = 0.1\n",
      "Iteration 3: norm_delta = 1.67e-02, step_size = 0.9500, log_lik = -17069.51781, newton_decrement = 9.01e-02, seconds_since_start = 0.1\n",
      "Iteration 4: norm_delta = 9.43e-04, step_size = 1.0000, log_lik = -17069.42770, newton_decrement = 2.65e-04, seconds_since_start = 0.2\n",
      "Iteration 5: norm_delta = 2.21e-07, step_size = 1.0000, log_lik = -17069.42744, newton_decrement = 2.14e-11, seconds_since_start = 0.2\n",
      "Convergence completed after 5 iterations.\n"
     ]
    }
   ],
   "source": [
    "for cluster_label in range(3):\n",
    "    interval_cluster = intervals_censored_black_male[intervals_censored_black_male['cluster_label'] == cluster_label]\n",
    "    res = optimal_regimen(interval_cluster)\n",
    "    res.to_csv(f'black_male/label={cluster_label}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
